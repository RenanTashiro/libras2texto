{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "\n",
    "\n",
    "image_shape = (128, 128, 3)\n",
    "\n",
    "features_extractor = MobileNet(include_top=False, input_shape=image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 12, 4, 4, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 12, 16384)         0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 256)               12780288  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                6156      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 16,146,892\n",
      "Trainable params: 12,918,028\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "\n",
    "def get_model(extractor, nb_classes=12, nb_frames=12, image_shape=(128, 128, 3), summary=False):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(TimeDistributed(extractor, input_shape=(nb_frames, *image_shape)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    model.add(GRU(256, activation=None, return_sequences=False))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.layers[0].trainable = False\n",
    "    \n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(features_extractor, image_shape=image_shape, summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=SGD(lr=0.001), # lr=0.01, nesterov=True\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import (\n",
    "    random_shift, \n",
    "    random_shear, \n",
    "    random_zoom, \n",
    "    flip_axis, \n",
    "    transform_matrix_offset_center,\n",
    "    apply_transform\n",
    ")\n",
    "\n",
    "\n",
    "# assume que as imagens sempre serao `last_channel`\n",
    "\n",
    "def random_seq_rotation(X, rg, row_axis=1, col_axis=2, channel_axis=0,\n",
    "                    fill_mode='nearest', cval=0.):\n",
    "    \"\"\"codigo modificado de `keras.preprocessing.image.random_rotation.\"\"\"\n",
    "    theta = np.deg2rad(np.random.uniform(-rg, rg))\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "    rX = []\n",
    "    \n",
    "    for x in X:\n",
    "        h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "        transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n",
    "        rX.append(apply_transform(x, transform_matrix, channel_axis, fill_mode, cval))\n",
    "                                  \n",
    "    return np.array(rX)\n",
    "\n",
    "def random_seq_shift(X, wrg, hrg, row_axis=1, col_axis=2, channel_axis=0,\n",
    "                 fill_mode='nearest', cval=0.):\n",
    "    \"\"\"codigo modificado de `keras.preprocessing.image.random_shift.\"\"\"\n",
    "    rX = []\n",
    "    dx = np.random.uniform(-hrg, hrg)\n",
    "    dy = np.random.uniform(-wrg, wrg)\n",
    "    \n",
    "    for x in X:\n",
    "        h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "        tx = dx * h\n",
    "        ty = dy * w\n",
    "        translation_matrix = np.array([[1, 0, tx],\n",
    "                                       [0, 1, ty],\n",
    "                                       [0, 0, 1]])\n",
    "        \n",
    "        transform_matrix = translation_matrix  # no need to do offset\n",
    "        rX.append(apply_transform(x, transform_matrix, channel_axis, fill_mode, cval))\n",
    "        \n",
    "    return np.array(rX)\n",
    "\n",
    "def random_seq_shear(X, intensity, row_axis=1, col_axis=2, channel_axis=0,\n",
    "                 fill_mode='nearest', cval=0.):\n",
    "    \"\"\"codigo modificado de `keras.preprocessing.image.random_shear.\"\"\"\n",
    "    shear = np.deg2rad(np.random.uniform(-intensity, intensity))\n",
    "    shear_matrix = np.array([[1, -np.sin(shear), 0],\n",
    "                             [0, np.cos(shear), 0],\n",
    "                             [0, 0, 1]])\n",
    "\n",
    "    rX = []\n",
    "    \n",
    "    for x in X:\n",
    "        h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "        transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n",
    "        rX.append(apply_transform(x, transform_matrix, channel_axis, fill_mode, cval))\n",
    "        \n",
    "    return np.array(rX)\n",
    "\n",
    "def random_seq_zoom(X, zoom_range, row_axis=1, col_axis=2, channel_axis=0,\n",
    "                fill_mode='nearest', cval=0.):\n",
    "    \"\"\"codigo modificado de `keras.preprocessing.image.random_zoom.\"\"\"\n",
    "    if len(zoom_range) != 2:\n",
    "        raise ValueError('`zoom_range` should be a tuple or list of two'\n",
    "                         ' floats. Received: ', zoom_range)\n",
    "\n",
    "    if zoom_range[0] == 1 and zoom_range[1] == 1:\n",
    "        zx, zy = 1, 1\n",
    "    else:\n",
    "        zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n",
    "    zoom_matrix = np.array([[zx, 0, 0],\n",
    "                            [0, zy, 0],\n",
    "                            [0, 0, 1]])\n",
    "\n",
    "    rX = []\n",
    "    \n",
    "    for x in X:\n",
    "        h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "        transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n",
    "        rX.append(apply_transform(x, transform_matrix, channel_axis, fill_mode, cval))\n",
    "        \n",
    "    return np.array(rX)\n",
    "\n",
    "\n",
    "def rotation(img, rg):\n",
    "    \"\"\"rg em graus.\"\"\"\n",
    "    return random_seq_rotation(img, rg, row_axis=0, col_axis=1, channel_axis=2)\n",
    "\n",
    "def shift(img, wrg, hrg):\n",
    "    \"\"\"rg entre [0,1].\"\"\"\n",
    "    return random_seq_shift(img, wrg, hrg, row_axis=0, col_axis=1, channel_axis=2)\n",
    "\n",
    "def shear(img, intensity):\n",
    "    \"\"\"intensity em graus.\"\"\"\n",
    "    return random_seq_shear(img, intensity, row_axis=0, col_axis=1, channel_axis=2)\n",
    "\n",
    "def zoom(img, zrg):\n",
    "    \"\"\"zrg entre [0.1].\"\"\"\n",
    "    return random_seq_zoom(img, (1.0-zrg, 1.0+zrg), row_axis=0, col_axis=1, channel_axis=2)\n",
    "\n",
    "def flip(img):\n",
    "    \"\"\"reflete a imagem em relacao ao eixo y.\"\"\"\n",
    "    return flip_axis(img, axis=1)\n",
    "\n",
    "def interval_mapping(image, from_min, from_max, to_min, to_max):\n",
    "    from_range = from_max - from_min\n",
    "    to_range = to_max - to_min\n",
    "    scaled = np.array((image - from_min) / float(from_range), dtype=float)\n",
    "    return to_min + (scaled * to_range)\n",
    "\n",
    "def read_video(video_path, size=(128, 128, 3)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    frames = []\n",
    "    \n",
    "    while ret:\n",
    "        if frame.shape != size:\n",
    "            frame = cv2.resize(frame, (size[1], size[0]))\n",
    "        frames.append(frame)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def preprocess(video_path, \n",
    "               nb_frames=12, \n",
    "               size=(128, 128, 3), \n",
    "               start_offset_rg=5, \n",
    "               rtrg=15, \n",
    "               wrg=0.15, \n",
    "               hrg=0.12, \n",
    "               shrg=25, \n",
    "               zrg=0.15,\n",
    "               toflip=True):\n",
    "    video = []\n",
    "    \n",
    "    if toflip and random.randint(0, 1) == 0:\n",
    "        toflip = False\n",
    "        \n",
    "    for frame in read_video(video_path, size=size):\n",
    "        frame = preprocess_input(frame.astype(np.float64))\n",
    "#         if toflip:\n",
    "#             flip(frame)\n",
    "        video.append(frame)\n",
    "        \n",
    "    start_offset = random.randint(0, start_offset_rg)\n",
    "    video = [frame for frame in video[start_offset::(len(video)-start_offset)//nb_frames]]\n",
    "    \n",
    "    while len(video) < nb_frames:\n",
    "        video.append(np.zeros(size))\n",
    "\n",
    "    ret =  np.array(video[:nb_frames])   \n",
    "        \n",
    "#     if rtrg != 0:\n",
    "#         ret = rotation(ret, rtrg)\n",
    "    if wrg != 0.0 or hrg != 0.0:\n",
    "        ret = shift(ret, wrg, hrg)\n",
    "#     if shrg != 0:\n",
    "#         ret = shear(ret, shrg)\n",
    "    if zrg != 0:\n",
    "        ret = zoom(ret, zrg)\n",
    "\n",
    "    return ret    \n",
    "    \n",
    "def generator(features, labels, batch_size, nb_frames=12, size=(128, 128, 3)):\n",
    "    batch_features = np.zeros((batch_size, nb_frames, *size))\n",
    "    batch_labels = np.zeros((batch_size, len(labels)))\n",
    "    \n",
    "    random.shuffle(features)\n",
    "    lock = threading.Lock()\n",
    "    index = 0\n",
    "    \n",
    "    # not thread-safe\n",
    "    def pick(features):\n",
    "        nonlocal index\n",
    "        with lock:\n",
    "            if index == len(features):\n",
    "                random.shuffle(features)\n",
    "                index = 0\n",
    "            feature = features[index]\n",
    "            index += 1\n",
    "        return feature\n",
    "\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            feature, label = pick(features)\n",
    "            batch_features[i] = preprocess(feature, size=size)\n",
    "            batch_labels[i] = labels[label]\n",
    "\n",
    "        yield batch_features, batch_labels\n",
    "\n",
    "def load(dataset_path):\n",
    "    videos_path = []\n",
    "\n",
    "    for root, subdir, filesname in os.walk(dataset_path):\n",
    "        for filename in filesname:\n",
    "            if '.mp4' in filename:\n",
    "                videos_path.append(os.path.join(root, filename))\n",
    "\n",
    "    _class = ['agosto', 'avisar', 'avisar-me', 'branco', 'educado', 'entender', \n",
    "              'entender-não', 'esquecer', 'pessoa', 'quente', 'rápido', 'sentimento']\n",
    "\n",
    "    features = []\n",
    "    labels = to_categorical(range(len(_class)))\n",
    "\n",
    "    for i, c in enumerate(_class):\n",
    "        for path in videos_path:\n",
    "            if c + '/' in path:\n",
    "                features.append((path, i))\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "dataset_path = 'dataset/training'\n",
    "validation_path = 'dataset/validation'\n",
    "batch_size = 1\n",
    "\n",
    "features, labels = load(dataset_path)\n",
    "vfeatures, vlabels = load(validation_path)\n",
    "\n",
    "gen = generator(features, labels, batch_size, size=image_shape)\n",
    "vgen = generator(vfeatures, vlabels, batch_size, size=image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 182 ms, sys: 16.7 ms, total: 199 ms\n",
      "Wall time: 82.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# debugar se esta gerando corretamenta as imagens\n",
    "batch, label = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(batch.shape, label.shape)\n",
    "\n",
    "img = np.concatenate([interval_mapping(img, np.min(img), np.max(img), 0, 255).astype(np.uint8)\n",
    "                for img in batch[0]], axis=1)\n",
    "\n",
    "plt.figure(figsize=(80,40))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:2095: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 205/6451 [..............................] - ETA: 16:06 - loss: 2.5295 - acc: 0.1659"
     ]
    }
   ],
   "source": [
    "# first test\n",
    "history = model.fit_generator(gen, \n",
    "                              steps_per_epoch=len(features)/batch_size,\n",
    "                              validation_data=vgen,\n",
    "                              validation_steps=len(vfeatures)/batch_size,\n",
    "                              epochs=10,\n",
    "                              max_queue_size=20,\n",
    "                              workers=3,\n",
    "                              use_multiprocessing=True) # test-only\n",
    "# max_queue_size=20\n",
    "# workers=2\n",
    "# use_multiprocessing=True\n",
    "\n",
    "# shear and zoom and diff_start_frame\n",
    "# ~24 -> ~30 -> ~36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testa na mao algumas previsoes\n",
    "_class = np.array(['agosto', 'avisar', 'avisar-me', 'branco', 'educado', 'entender', \n",
    "          'entender-não', 'esquecer', 'pessoa', 'quente', 'rápido', 'sentimento'])\n",
    "\n",
    "batch, label = next(gen)\n",
    "predict = model.predict(batch)\n",
    "\n",
    "print('predict', _class[np.argmax(predict, axis=1)])\n",
    "print('labels ', _class[np.argmax(label, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiza um video\n",
    "img = np.concatenate([interval_mapping(img, np.min(img), np.max(img), 0, 255).astype(np.uint8)\n",
    "                for img in batch[0]], axis=1)\n",
    "\n",
    "plt.figure(figsize=(80,40))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log\n",
    "\n",
    "### LSTM(256) -> Dense(512) -> Dense(12)\n",
    "\n",
    "#### Um voluntario, uma camera, >90%\n",
    "\n",
    "#### SGD(0.001), Doze voluntarios, cam8, 97.63%, 8 frames\n",
    "```\n",
    "Epoch 1/10\n",
    "717/717 [==============================] - 80s 112ms/step - loss: 2.3100 - acc: 0.2092\n",
    "Epoch 2/10\n",
    "717/717 [==============================] - 63s 88ms/step - loss: 1.7178 - acc: 0.4533\n",
    "Epoch 3/10\n",
    "717/717 [==============================] - 63s 87ms/step - loss: 1.2424 - acc: 0.6318\n",
    "Epoch 4/10\n",
    "717/717 [==============================] - 63s 87ms/step - loss: 0.9182 - acc: 0.7378\n",
    "Epoch 5/10\n",
    "717/717 [==============================] - 62s 87ms/step - loss: 0.6590 - acc: 0.8243\n",
    "Epoch 6/10\n",
    "717/717 [==============================] - 62s 87ms/step - loss: 0.4894 - acc: 0.8926\n",
    "Epoch 7/10\n",
    "717/717 [==============================] - 62s 87ms/step - loss: 0.3579 - acc: 0.9331\n",
    "Epoch 8/10\n",
    "717/717 [==============================] - 62s 87ms/step - loss: 0.2697 - acc: 0.9540\n",
    "Epoch 9/10\n",
    "717/717 [==============================] - 62s 86ms/step - loss: 0.2101 - acc: 0.9609\n",
    "Epoch 10/10\n",
    "717/717 [==============================] - 62s 87ms/step - loss: 0.1690 - acc: 0.9763\n",
    "```\n",
    "\n",
    "#### SGD, Doze voluntarios, cam[3,8], training[99.93%], validation[45,83%], 8 frames => overfitting\n",
    "\n",
    "Provavelmente estava refinando dos treinos anteriores\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "1435/1435 [==============================] - 135s 94ms/step - loss: 0.7159 - acc: 0.7624\n",
    "Epoch 2/10\n",
    "1435/1435 [==============================] - 124s 86ms/step - loss: 0.4075 - acc: 0.8850\n",
    "Epoch 3/10\n",
    "1435/1435 [==============================] - 127s 89ms/step - loss: 0.2650 - acc: 0.9345\n",
    "Epoch 4/10\n",
    "1435/1435 [==============================] - 125s 87ms/step - loss: 0.1867 - acc: 0.9547\n",
    "Epoch 5/10\n",
    "1435/1435 [==============================] - 133s 93ms/step - loss: 0.1240 - acc: 0.9770\n",
    "Epoch 6/10\n",
    "1435/1435 [==============================] - 127s 88ms/step - loss: 0.0961 - acc: 0.9868\n",
    "Epoch 7/10\n",
    "1435/1435 [==============================] - 127s 89ms/step - loss: 0.0815 - acc: 0.9895\n",
    "Epoch 8/10\n",
    "1435/1435 [==============================] - 125s 87ms/step - loss: 0.0640 - acc: 0.9958\n",
    "Epoch 9/10\n",
    "1435/1435 [==============================] - 127s 89ms/step - loss: 0.0590 - acc: 0.9951\n",
    "Epoch 10/10\n",
    "1435/1435 [==============================] - 125s 87ms/step - loss: 0.0344 - acc: 0.9993\n",
    "```\n",
    "\n",
    "#### SGD, Doze voluntarios, cam[all], training[99.58%], validation[38.70%], 8 frames\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "6451/6451 [==============================] - 623s 97ms/step - loss: 0.8525 - acc: 0.6800\n",
    "Epoch 2/10\n",
    "6451/6451 [==============================] - 573s 89ms/step - loss: 0.3967 - acc: 0.8613\n",
    "Epoch 3/10\n",
    "6451/6451 [==============================] - 589s 91ms/step - loss: 0.2715 - acc: 0.9054\n",
    "Epoch 4/10\n",
    "6451/6451 [==============================] - 563s 87ms/step - loss: 0.1727 - acc: 0.9443\n",
    "Epoch 5/10\n",
    "6451/6451 [==============================] - 608s 94ms/step - loss: 0.1247 - acc: 0.9630\n",
    "Epoch 6/10\n",
    "6451/6451 [==============================] - 579s 90ms/step - loss: 0.0860 - acc: 0.9760\n",
    "Epoch 7/10\n",
    "6451/6451 [==============================] - 570s 88ms/step - loss: 0.0670 - acc: 0.9833\n",
    "Epoch 8/10\n",
    "6451/6451 [==============================] - 575s 89ms/step - loss: 0.0460 - acc: 0.9882\n",
    "Epoch 9/10\n",
    "6451/6451 [==============================] - 571s 89ms/step - loss: 0.0389 - acc: 0.9930\n",
    "Epoch 10/10\n",
    "6451/6451 [==============================] - 574s 89ms/step - loss: 0.0288 - acc: 0.9958\n",
    "```\n",
    "\n",
    "#### Adam, Dropout(0.5), All, training[<10%], 12 frames\n",
    "\n",
    "```\n",
    "Epoch 1/15\n",
    "6451/6451 [==============================] - 789s 122ms/step - loss: 2.5006 - acc: 0.0758\n",
    "Epoch 2/15\n",
    "6451/6451 [==============================] - 776s 120ms/step - loss: 2.4932 - acc: 0.0809\n",
    "Epoch 3/15\n",
    "6451/6451 [==============================] - 775s 120ms/step - loss: 2.4878 - acc: 0.0837\n",
    "Epoch 4/15\n",
    "6451/6451 [==============================] - 777s 120ms/step - loss: 2.4870 - acc: 0.0815\n",
    "Epoch 5/15\n",
    "6451/6451 [==============================] - 778s 121ms/step - loss: 2.4857 - acc: 0.0882\n",
    "Epoch 6/15\n",
    "6451/6451 [==============================] - 775s 120ms/step - loss: 2.4850 - acc: 0.0873\n",
    "Epoch 7/15\n",
    "6451/6451 [==============================] - 784s 121ms/step - loss: 2.4852 - acc: 0.0862\n",
    "Epoch 8/15\n",
    "6451/6451 [==============================] - 786s 122ms/step - loss: 2.4849 - acc: 0.0842\n",
    "Epoch 9/15\n",
    " 448/6451 [=>............................] - ETA: 12:22 - loss: 2.4856 - acc: 0.0714\n",
    "```\n",
    "\n",
    "#### 12 frames, from-scratch, SGD, Dropout[LSTM=0.25], All, training[82.63%], validation[26,31%] porem dobrou a quantidade de exemplos\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "1613/1612 [==============================] - 563s 349ms/step - loss: 2.1882 - acc: 0.2257\n",
    "Epoch 2/10\n",
    "1613/1612 [==============================] - 561s 348ms/step - loss: 1.5695 - acc: 0.4405\n",
    "Epoch 3/10\n",
    "1613/1612 [==============================] - 560s 347ms/step - loss: 1.1912 - acc: 0.5635\n",
    "Epoch 4/10\n",
    "1613/1612 [==============================] - 561s 348ms/step - loss: 0.9761 - acc: 0.6303\n",
    "Epoch 5/10\n",
    "1613/1612 [==============================] - 561s 348ms/step - loss: 0.8422 - acc: 0.6835\n",
    "Epoch 6/10\n",
    "1613/1612 [==============================] - 560s 347ms/step - loss: 0.7449 - acc: 0.7170\n",
    "Epoch 7/10\n",
    "1613/1612 [==============================] - 561s 348ms/step - loss: 0.6543 - acc: 0.7557\n",
    "Epoch 8/10\n",
    "1613/1612 [==============================] - 560s 347ms/step - loss: 0.5824 - acc: 0.7852\n",
    "Epoch 9/10\n",
    "1613/1612 [==============================] - 561s 348ms/step - loss: 0.5294 - acc: 0.8035\n",
    "Epoch 10/10\n",
    "1613/1612 [==============================] - 570s 354ms/step - loss: 0.4847 - acc: 0.8263\n",
    "```\n",
    "\n",
    "#### 12f, SGD[0.001], LSTM[256, 0.2], training[], validation[], data augumentation[15,0.15,0.10,25,0.15,1]\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "1613/1612 [==============================] - 2314s 1s/step - loss: 2.3825 - acc: 0.1496 - val_loss: 2.4080 - val_acc: 0.1102\n",
    "Epoch 2/10\n",
    "1613/1612 [==============================] - 2161s 1s/step - loss: 2.0039 - acc: 0.2621 - val_loss: 2.4029 - val_acc: 0.1287\n",
    "Epoch 3/10\n",
    "1613/1612 [==============================] - 2159s 1s/step - loss: 1.7697 - acc: 0.3329 - val_loss: 2.3915 - val_acc: 0.1139\n",
    "Epoch 4/10\n",
    "1613/1612 [==============================] - 2159s 1s/step - loss: 1.5794 - acc: 0.3952 - val_loss: 2.7267 - val_acc: 0.1222\n",
    "Epoch 5/10\n",
    "1613/1612 [==============================] - 2160s 1s/step - loss: 1.4523 - acc: 0.4382 - val_loss: 2.5037 - val_acc: 0.1556\n",
    "Epoch 6/10\n",
    "1613/1612 [==============================] - 2161s 1s/step - loss: 1.3468 - acc: 0.4747 - val_loss: 2.6726 - val_acc: 0.1435\n",
    "Epoch 7/10\n",
    "1613/1612 [==============================] - 2196s 1s/step - loss: 1.2705 - acc: 0.4966 - val_loss: 2.8907 - val_acc: 0.1352\n",
    "Epoch 8/10\n",
    "1613/1612 [==============================] - 2188s 1s/step - loss: 1.2373 - acc: 0.5143 - val_loss: 2.9156 - val_acc: 0.1250\n",
    "Epoch 9/10\n",
    "1613/1612 [==============================] - 2167s 1s/step - loss: 1.1876 - acc: 0.5271 - val_loss: 3.2720 - val_acc: 0.1269\n",
    "Epoch 10/10\n",
    " 539/1612 [=========>....................] - ETA: 20:21 - loss: 1.1547 - acc: 0.5404\n",
    "```\n",
    "\n",
    "#### SGD(lr=0.001), GRU(256), Dense(512), Dense(12)\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "6451/6451 [==============================] - 682s 106ms/step - loss: 1.3972 - acc: 0.4703 - val_loss: 1.5946 - val_acc: 0.3565\n",
    "Epoch 2/10\n",
    "6451/6451 [==============================] - 687s 106ms/step - loss: 0.7463 - acc: 0.6963 - val_loss: 1.7637 - val_acc: 0.3417\n",
    "Epoch 3/10\n",
    "6451/6451 [==============================] - 707s 110ms/step - loss: 0.5602 - acc: 0.7779 - val_loss: 1.8590 - val_acc: 0.3491\n",
    "Epoch 4/10\n",
    "6451/6451 [==============================] - 691s 107ms/step - loss: 0.4699 - acc: 0.8127 - val_loss: 1.8250 - val_acc: 0.3713\n",
    "Epoch 5/10\n",
    "6451/6451 [==============================] - 677s 105ms/step - loss: 0.4087 - acc: 0.8465 - val_loss: 2.4513 - val_acc: 0.3231\n",
    "Epoch 6/10\n",
    "6451/6451 [==============================] - 668s 104ms/step - loss: 0.4035 - acc: 0.8439 - val_loss: 1.9002 - val_acc: 0.3583\n",
    "Epoch 7/10\n",
    "6451/6451 [==============================] - 668s 104ms/step - loss: 0.3711 - acc: 0.8565 - val_loss: 2.5339 - val_acc: 0.3259\n",
    "Epoch 8/10\n",
    "6451/6451 [==============================] - 668s 104ms/step - loss: 0.3087 - acc: 0.8845 - val_loss: 2.2885 - val_acc: 0.3815\n",
    "Epoch 9/10\n",
    "6451/6451 [==============================] - 666s 103ms/step - loss: 0.2977 - acc: 0.8876 - val_loss: 2.5253 - val_acc: 0.3509\n",
    "Epoch 10/10\n",
    "6451/6451 [==============================] - 669s 104ms/step - loss: 0.2926 - acc: 0.8902 - val_loss: 2.7634 - val_acc: 0.3676\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
